cpt embedding file: /data/xxin/workspace/CptODEE_new/data/cpt_embeddings_in_bert
cpt vec length: 34443
training corpus type: /data/yjl/cpt_prep/train_data/cached_span_sample_without_mlm
training data length: 1	testing data length: 5
learning rate: 0.0001
warmup_ratio: 0.05
batch size: 1
gradient_accumulation_steps: 1
train_epoch_num: 500
negative sampled size: 200
loading configuration file /data/yjl/cpt_prep/pretrain_model/config.json
Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}

loading weights file /data/yjl/cpt_prep/pretrain_model/pytorch_model.bin
All model checkpoint weights were used when initializing BertModel.

All the weights of BertModel were initialized from the model checkpoint at /data/yjl/cpt_prep/pretrain_model.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
step 0 / 1 in total 1 of epoch 0, train/loss: 3.167130947113037

************  pause in step : 1, start to test evaluation.  *********

