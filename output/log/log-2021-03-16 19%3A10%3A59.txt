cpt embedding file: /data/xxin/workspace/CptODEE_new/data/cpt_embeddings_in_bert
cpt vec length: 34443
training corpus type: /data/yjl/cpt_prep/train_data/cached_span_sample_without_mlm
training data length: 10	testing data length: 5
learning rate: 0.0001
warmup_ratio: 0.05
batch size: 5
gradient_accumulation_steps: 1
train_epoch_num: 500
negative sampled size: 200
loading configuration file /data/yjl/cpt_prep/pretrain_model/config.json
Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}

loading weights file /data/yjl/cpt_prep/pretrain_model/pytorch_model.bin
All model checkpoint weights were used when initializing BertModel.

All the weights of BertModel were initialized from the model checkpoint at /data/yjl/cpt_prep/pretrain_model.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
step 0 / 2 in total 10 of epoch 0, train/loss: 2.5509753227233887

step 1 / 2 in total 10 of epoch 0, train/loss: 2.375552177429199

************  pause in step : 2, start to test evaluation.  *********


test data check! 

F1: -1.00, P: -1.00, R:-1.00
step 0 / 2 in total 10 of epoch 1, train/loss: 2.3608765602111816

step 1 / 2 in total 10 of epoch 1, train/loss: 2.0702602863311768

************  pause in step : 4, start to test evaluation.  *********


test data check! 

F1: -1.00, P: -1.00, R:-1.00
